I wasn't able to reproduce the authors' Figure 2.4
of the average reward of UCB vs. epsilon-greedy bandit
action selection using their parameters.
Using other parameters, I did observe the same
behavior.

For the parameter study, I used 200, not 2000,
runs/environments for each actor and parameter set.
It was just faster. 
Writing this also helped me realize that there were
at least 4 parameters I didn't vary and that the 
authors didn't mention explicitly. Parameter search
takes a while.
In the general case, when expected rewards were not
centered around zero, I found the gradient-based
methods to work best on this simple bandit problem.
But epsilon-greedy with optimistic initialization 
worked surprisingly well, too.
These results are, of course, only a measure of
average performance, not the eventual best
performance, over the first 1000 time steps.